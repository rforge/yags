%\VignetteIndexEntry{Notes for eSet developers}
%\VignetteDepends{}
%\VignetteKeywords{Expression Analysis}
%\VignettePackage{Biobase}


%
% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is
% likely to be overwritten.
%
\documentclass[12pt]{article}

\usepackage{amsmath,pstricks}
\usepackage[authoryear,round]{natbib}
\usepackage{hyperref}


\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}


\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}

\textwidth=6.2in

\bibliographystyle{plainnat} 
 
\begin{document}
%\setkeys{Gin}{width=0.55\textwidth}

\title{Illustrating doubly scalable statistical computing
strategies}
\author{VJ Carey}
\maketitle

\tableofcontents

\clearpage

\section{Introduction}

Performant, interactive statistical
analysis with very large data on multicore gigaflop, gigaRAM hardware
requires programming strategies that are not necessarily familiar
to statistical methodologists.
In this note, simple approaches to decomposing large data
and data analytic computations are reviewed.

\section{A data provider model}

To keep things simple, we will not use examples involving
external data resources with unusual formats.  One
data frame with 5 million records will serve to illustrate
the basic strategy.

<<lkp>>=
library(geeni)
data(bigd)
dim(bigd)
bigd[1:4,]
@

The following function returns a function that
will return a sequentially specified chunk of any R object that answers
to \texttt{nrow()} and returns a data frame instance
when a row request is made with \texttt{[}.
To avoid dealing with large footprints of indexing
vectors, we use the \textit{ff} package's hierarchical
indexing objects to define chunks; eventually these will
convert to integer index vectors, but this is deferred until
needed.

<<mkc,keep.source=TRUE>>=
library(ff)
chunkable = function(data, n=NULL, chunksize=10) {
  # don't use a cursor but define an index:
  # stateless query resolution
  require(bit)
  if (is.null(n)) n= nrow(data)
  inds = chunk(from=1, to=n, by=chunksize, maxindex=n)
  datafun <- function(which) {
    if (which < 1 | which > length(inds)) return(NULL)
    if (which > length(inds)) stop("request for nonexistent chunk")
    tin = function(x) as.integer(as.hi(x))
    data[ tin(inds[[which]]), ]
    }
}
@

We will not use \texttt{chunkable} directly,
but will create a reference class to manage the chunk queries.

<<mkrc,keep.source=TRUE>>=
.dfbychunk = setRefClass("dfbychunk", 
  fields = list(chunksize="integer", nchunk="integer",
    data="data.frame"),
  methods = list(
    show=function(...) cat(nrow(data), "x", ncol(data), 
          "dfbychunk instance, chunksize", chunksize, "\n"),
    getchunk = function(which) 
        chunkable(data, chunksize=chunksize)(which),
    initialize = function(..., chunksize, data) {
       # do we really need this?
        if (missing(chunksize)) chunksize <<- 10L
        else chunksize <<- chunksize
        if (!missing(data)) data <<- data
        nchunk <<- as.integer(ceiling(nrow(data)/chunksize))
        }
))
@

\section{A scalable implementation of Fisher scoring}

The following code works with the chunkable data reference
class (instance to be supplied as \texttt{dfbyc}) to
decompose data and computations leading to an iteration
called Fisher scoring.  In this case, we have $Ey = X\beta$,
and the chunked partition
into $C$ chunks $X_i$, $y_i$, $i = 1,\ldots, C$ leads to the iterative computation of $\hat{\beta}$
as $\beta^{(k+1)} = \beta^{(k)} + 
  (\sum X_i^t X_i)^{-1}(\sum X_i^t(y_i - X_i\beta^{(k)})$, which will
converge under regularity conditions.  The chunking
keeps memory requirements small, and the statelessness of the
partition elements allows independent computation of contributors
to the sum.

<<doinfra,keep.source=TRUE>>=
library(foreach)
combi = function (x, y)
   lapply(1:length(x), function(i) x[[i]] + y[[i]])
parols <- function (fmla, dfbyc, family, 
                     binit, maxit = 20, tol = 1e-06) 
{
    beta = binit
    del = Inf
    curit = 1
    nchunk = dfbyc$nchunk
    while (max(abs(del)) > tol) {
        delcomp = foreach(i = 1:nchunk, .combine = combi) %dopar% {
            fm = model.frame(fmla, dfbyc$getchunk(i))
            xi = model.matrix(fmla, fm)
            yi = model.response(fm)
	    resi = yi - xi%*%beta
            list(xpx = t(xi)%*%xi, xpr = t(xi)%*%resi, 
                      ssr=sum(resi^2), ni=length(yi))
            }
        xpxinv = solve(delcomp[[1]])
        del = xpxinv %*% delcomp[[2]]
        beta = beta + del
        curit = curit + 1
        if (curit > maxit) 
            stop(paste("maxit [", maxit, "] iterations exceeded"))
    }
    ssr = delcomp[[3]]
    N = delcomp[[4]]
    se = sqrt(diag((ssr/(N-length(del)))*xpxinv))
    list(stats=cbind(beta,se), ssr=ssr, N=N)
}
@

\section{Setting up the data provider}

Given the large data frame, we create the chunking provider
as
<<setup>>=
bigprov = .dfbychunk$new( data=bigd, chunksize = as.integer(5e6/8) )
dim(bigprov$getchunk(1))
@

\section{Invoking the doubly scalable procedure}

We will use 8 cores.
<<docalc>>=
library(doMC)
registerDoMC(cores=8)
ut1 = unix.time(p1 <- parols( isT~GCcon, bigprov, gaussian(), binit=c(0,0) ))
ut1
p1
@

Compare the naive solution:
<<dolm>>=
ut2 = unix.time(l1 <- lm(isT~GCcon, data=bigd))
summary(l1)
@

\section{Comparison to a space-efficient solution}

We need a little infrastructure to deploy \texttt{biglm}
in a similar manner.  Again we try to use low impact indexing.

<<mkch>>=
library(biglm)
choppedlm = function(fmla, data, chunks) {
  ai = function(x) as.integer(as.hi(x))
  m = biglm(fmla, data[ai(chunks[[1]]),])
  if (length(chunks)==1) return(m)
  for (j in 2:length(chunks)) m = update(m, bigd[ai(chunks[[j]]),])
  m
}
@
We create 5 chunks of size 1M each:
<<getc>>=
c1M = chunk(from=1, to=5e6, by=1e6, max=5e6)
@
and run the procedure, concluding with comparison.
<<dobiglm>>=
ut3 = unix.time(b1 <- choppedlm(isT~GCcon, bigd, c1M))
summary(b1)
list(ut1, ut2, ut3)
@



\end{document}
